{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conversion of Words to Numbers:\n",
    "\n",
    "##### Embedding Techniques: \n",
    "Each word in the input sequence is mapped to a vector of real numbers using embedding techniques like Word2Vec, GloVe, or learned embeddings within the transformer itself.\n",
    "\n",
    "\n",
    "##### Positional Embeddings: \n",
    "In addition to the word embeddings, positional embeddings are added to provide information about the position of each word in the sequence. This is crucial for capturing the order of words since transformers process the entire sequence simultaneously and do not inherently understand the order.\n",
    "\n",
    "#### Vector Representation:\n",
    "\n",
    "The resulting numerical vectors (word embeddings combined with positional embeddings) are then fed into the transformer model. These vectors serve as the input that the transformer  uses for all subsequent processing steps, including attention mechanisms and layer transformations.\n",
    "This process of converting words into numerical representations allows the transformer to handle and process sequences of text effectively, enabling it to perform tasks such as translation, summarization, and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ImputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model:int, vocab_size:int):\n",
    "    # Calls the constructor of the parent class (nn.Module)\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        # Creates an embedding layer that maps vocabulary \n",
    "        # indices to dense vectors and these vectors\n",
    "        # are learned by the model during training.\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Applies the embedding layer to the input and scales the embeddings\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)\n",
    "    \n",
    "## Positional Encoding\n",
    "\n",
    "## Remember that the original sentence is mapped to a list of vectors by the \n",
    "## embedding layers. We want to encode the position of every word in the \n",
    "## sentence, and this is done by adding another vector of the same size as the \n",
    "## embedding that tells the model the position occupied by each word.\n",
    "\n",
    "\n",
    "# This can be done in a clever way, in the paper \"Attention is all you need\", they used sinusoidal\n",
    "# function, this has great advantages, in the very dinamical way. it step or every position can be cover\n",
    "# in a simple discretized way function and partionized for bigs numbers\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model:int, seq_len:int,dropout:float)->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # A matrix of zeros is created with a shape of \n",
    "        # (seq_len, d_model) to store positional encodings.\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        \n",
    "        ## Create a vector of shape (seq_len,1)\n",
    "           \n",
    "        # A position vector is created with values \n",
    "        # ranging from 0 to seq_len - 1. \n",
    "        # This provides a unique positional representation for each position in the sequence.\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Division terms are calculated to be used in \n",
    "        # sinusoidal and cosinusoidal functions for generating positional encodings. as the paper says.\n",
    "   \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        #Notice that in the paper, the real div_term is a power of 10^4, the log representation, i.e. the log domain is less \n",
    "        #dense, this has computer adavantages, the result is the same so is recomended to used.\n",
    "        \n",
    "        # Apply the sin to even position\n",
    "       \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply the cos  ot odd   position\n",
    "\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # We need the batch dimention of the tensor,\n",
    "        #  An extra dimension is added to the tensor to represent \n",
    "        # the batch dimension. This ensures that positional encodings \n",
    "        # can be added consistently to each batch of input data.\n",
    "        pe = pe.unsqueeze(0) # Tehsor of the size (1,seq_len,dim_len)\n",
    "\n",
    "        #e positional encoding is registered as a \n",
    "        # buffer so that it's treated as a model parameter but\n",
    "        #  doesn't require gradients during training.\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adds the positional encodings to the input embeddings\n",
    "\n",
    "        #In the forward method, positional encodings are added to the\n",
    "        #  input embeddings to provide information about the position\n",
    "        #  of each element in the sequence. self.pe[:x.size(1), :] selects\n",
    "        #  positional encodings corresponding to the length of the input\n",
    "        #  sequence x. Additionally, requires_grad_(False) is used to ensure\n",
    "        #  that positional encodings are not trainable (don't require gradients).Because\n",
    "        #  are fixed.\n",
    "        #  Finally, the modified embeddings are returned after applying a dropout layer.\n",
    "\n",
    "        x = x + (self.pe[:x.size(1), :]).requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        # Small constant to avoid division by zero during normalization\n",
    "        self.eps = eps\n",
    "        # Learnable parameter for scaling (initialized to 1)\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # Scaling factor (gamma)\n",
    "        # Learnable parameter for shifting (initialized to 0)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # Bias term (beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the mean of the input tensor along the last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # Calculate the standard deviation of the input tensor along the last dimension\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # Normalize the input tensor, then scale and shift it using alpha and bias\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
