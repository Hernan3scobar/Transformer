{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "#### What is imput enbedding and how it is done?\n",
    "\n",
    "* The sentence called from now on token, it must be mapped into number.This is:  \" Your cat is a lovely cat\" it transform a list (Vector) of numbers (105 6587 5476 3558 54 5666) this number represent the position in a \"vocabulary\" as a id. We take this imput ID and map to a vector of size 512 (this is the size of the vector, i.e. 512 numbers) and we always map the same word to the same vector but, this vector its for training, this mean that the nubers will change. This is embbeding, to transform sentence into worlds\n",
    "\n",
    "d_model = 512, just like the paper \"Attention is all you need\"\n",
    "\n",
    "#### What is positional encoding\n",
    "\n",
    "* We want each word to carry some information about ist position in the sentence\n",
    "* We want that our model treat the word close as it was real close to each other and the distance word\n",
    "as it was a long distance\n",
    "* We want that the positional encodng to represent pattern that can be learned for the model,\n",
    "example:\n",
    "\n",
    "\"What is positional encoding\", in our vocabulary we know that \"what\" is before to encoding, and it has meaning where it is\n",
    "ubicated. So it necessary that our model can understand this and predict patter from it. \n",
    "\n",
    "The posittional encoding, it can be calculated from:\n",
    "* $PE(pos,2i)=\\sin \\frac{pos}{10000^{\\frac{2i}{d_{model}}}}$, for the odd\n",
    "\n",
    "* $PE(pos,2i+1)=\\cos \\frac{pos}{10000^{\\frac{2i}{d_{model}}}}$. for the nun\n",
    "\n",
    "why choose sinusoidal function: This is just in order the model can recognized patter. The trigonometric functino\n",
    "are a family of periodic function that haz patter in every aspect of nature.\n",
    "\n",
    "\n",
    "#####  What is Self- Attention\n",
    "Is a mechanism that exist before the paper, the self attention allow the model relate words to each other. It can be calculated from\n",
    "* $Attention(Q,K,V) = softmax\\left(\\frac{QK^{T}}{\\sqrt(d_{k})}\\right)V$\n",
    "\n",
    "Where this 3 matrix $Q, K, V $ are the embedding together with the positional encoding results, this matrix are of dimention $(seq,d_{model}=d_{k}=512)$, this are just the imput sequence.\n",
    "\n",
    "* notice: a soft max function has the property that every row oif a matrix A sum to 1:\n",
    "\n",
    "    * $row_{i}=\\sum_{j=1}^{512}{A_{ij}}=1$.\n",
    "\n",
    " and it represent how are related the word beteewn them.\n",
    "\n",
    "\n",
    "* in matrix multiplication, if we have a sequence of 6 word and we enbbeding the words to a vector of 512, the dimentions of \n",
    "    the matrix multiplication of $QK^{T}$ are:\n",
    "\n",
    "    \n",
    "    $(6,512)(512,6)=(6,6)$,\n",
    "\n",
    "\n",
    "* the matrix multiplication of $\\left(\\frac{QK^{T}}{\\sqrt(d_{k})}\\right)V$ are \n",
    "    $(6,6)(6,512)=(6,512)$ , \n",
    "\n",
    "it has the same dimentions of the input matrix, and it capture not just the meaning given by the embedding or the positional in the sentence, given by the possitional encoding, but also each wordÂ´s interaction whit other words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Mathematical Definition of Self-Attention and Permutation Invarianc\n",
    "\n",
    "\n",
    "Self-attention is a mechanism used in neural networks, especially in natural language processing, to assign different weights to different parts of the input sequence. The key feature is that it is permutation invariant, meaning the operation does not depend on the order of the input elements. Let's define this mathematically and prove its permutation invariance.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "  $P$ is a square matrix obtained by permuting the rows of an identity matrix. It contains exactly one entry of 1 in each row and each column, and 0s elsewhere. Here is an example of a generic permutation matrix $P$ of size $n \\times n$:\n",
    "\n",
    "  $$ P = \\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 0 & 0 & 0 & \\cdots & 0 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Alternatively, a permutation matrix can be written in a more general form as:\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "p_{11} & p_{12} & \\cdots & p_{1n} \\\\\n",
    "p_{21} & p_{22} & \\cdots & p_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{n1} & p_{n2} & \\cdots & p_{nn}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where each $p_{ij}$ is either 0 or 1, and each row and column contains exactly one entry of 1.\n",
    "\n",
    "\n",
    "### 1. Projections\n",
    "\n",
    "Given a set of input vectors \\(X\\):\n",
    "\n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "\n",
    "If we apply the permutation \\(P\\) to \\(X\\), we get:\n",
    "\n",
    "$$\n",
    "X' = PX\n",
    "$$\n",
    "\n",
    "Then, the new projections are:\n",
    "\n",
    "$$\n",
    "Q' = X'W_Q = (PX)W_Q = P(XW_Q) = PQ\n",
    "$$\n",
    "$$\n",
    "K' = X'W_K = (PX)W_K = P(XW_K) = PK\n",
    "$$\n",
    "$$\n",
    "V' = X'W_V = (PX)W_V = P(XW_V) = PV\n",
    "$$\n",
    "\n",
    "### 2. Attention Scores\n",
    "\n",
    "The attention matrix \\(A\\) is calculated as:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "If we apply the permutation, we have:\n",
    "\n",
    "$$\n",
    "A' = \\text{softmax}\\left(\\frac{Q'K'^T}{\\sqrt{d_k}}\\right) = \\text{softmax}\\left(\\frac{(PQ)(PK)^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "Let's develop this expression step by step:\n",
    "\n",
    "$$\n",
    "A' = \\text{softmax}\\left(\\frac{(PQ)(PK)^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "Recall that the transpose of a product is the product of the transposes in reverse order:\n",
    "\n",
    "$$\n",
    "(PK)^T = K^T P^T\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "A' = \\text{softmax}\\left(\\frac{P(QK^T)P^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "### Property of the Softmax\n",
    "\n",
    "The softmax function is applied row-wise, and if we permute the rows before and after applying softmax, we get the same result:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(P M P^T) = P \\cdot \\text{softmax}(M) \\cdot P^T\n",
    "$$\n",
    "\n",
    "Applying this property:\n",
    "\n",
    "$$\n",
    "A' = P \\cdot \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot P^T = P A P^T\n",
    "$$\n",
    "\n",
    "### 3. Self-Attention Output\n",
    "\n",
    "The output of the self-attention is calculated as:\n",
    "\n",
    "$$\n",
    "Z = AV\n",
    "$$\n",
    "\n",
    "For the permuted input:\n",
    "\n",
    "$$\n",
    "Z' = A' V' = (PAP^T)(PV) = PA(P^T P)V = PAV = PZ\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDncoder\n",
    "* Layer normalization\n",
    "\n",
    "Imagine we have a batch of 3 item, each of them could have different features, so we just calculated the variance and the mean  and normalized by:\n",
    "\n",
    "## $ \\hat{x}_{j} =\\frac{x_j - \\mu_j}{\\sigma^{2}_j+\\epsilon}$ \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODER\n",
    "\n",
    "# Layer Masked Multi-head Attention\n",
    "\n",
    "Our goal is to make the model more casual, this mean that the relation beetween words can only be affected by his order in the sentence, and in whit respect whit the previous word . this mean that the model can not be capable of seen word comming from the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
